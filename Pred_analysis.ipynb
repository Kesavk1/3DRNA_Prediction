# Kaggle Directory
DATA_DIR = os.getenv('DATA_DIR', '/kaggle/input/stanford-rna-3d-folding/')
main_files = [
    "train_sequences.csv", 
    "train_labels.csv", 
    "validation_sequences.csv", 
    "validation_labels.csv", 
    "test_sequences.csv",
    "sample_submission.csv"
]

DEFAULT_THRESHOLD = 0.4  # Default threshold after analysis

def optimize_dataframe(df, inplace=False, category_threshold=DEFAULT_THRESHOLD):
    """
    Optimizes the DataFrame to save memory.
    """
    if category_threshold < 0 or category_threshold > 1:
        raise ValueError("category_threshold must be between 0 and 1.")
    
    if not inplace:
        df = df.copy()
    
    for col in df.columns:
        col_type = df[col].dtype
        if np.issubdtype(col_type, np.integer):
            c_min, c_max = df[col].min(), df[col].max()
            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                df[col] = df[col].astype(np.int32)
        elif np.issubdtype(col_type, np.floating):
            if df[col].min() > np.finfo(np.float32).min and df[col].max() < np.finfo(np.float32).max:
                df[col] = df[col].astype(np.float32)
        if col_type == object:
            unique_vals = len(df[col].unique())
            if unique_vals / len(df) < category_threshold:
                df[col] = df[col].astype('category')
    
    return df

def load_main_data(chunksize=50000):
    """
    Loads the main files.
    """
    data = {}
    for file_name in main_files:
        file_path = os.path.join(DATA_DIR, file_name)
        if os.path.exists(file_path):
            chunks = pd.read_csv(file_path, on_bad_lines='skip', low_memory=False, chunksize=chunksize)
            dataframes = [optimize_dataframe(chunk, category_threshold=DEFAULT_THRESHOLD) for chunk in chunks]
            data[file_name] = pd.concat(dataframes, ignore_index=True)
        else:
            print(f"File {file_path} not found!")
    return data

def check_data_integrity(original_df, optimized_df):
    """
    Checks if the optimization did not alter the data.
    """
    try:
        pd.testing.assert_frame_equal(original_df, optimized_df, check_like=True)
        print("Integrity check passed: No changes in data after optimization.")
    except AssertionError as e:
        print(f"Data integrity check failed: {e}")

def check_duplicates(df):
    """
    Checks for duplicates in the DataFrame.
    """
    duplicates = df[df.duplicated(keep=False)]
    if not duplicates.empty:
        print(f"Warning: Duplicates found in the dataset. Number of duplicates: {duplicates.shape[0]}")
        return duplicates
    else:
        print("No duplicates found.")
    return None

def test_thresholds(df):
    """
    Tests different thresholds for DataFrame optimization.
    """
    thresholds = np.linspace(0.1, 0.9, 9)
    memory_usages = []
    for threshold in thresholds:
        optimized_df = optimize_dataframe(df.copy(), category_threshold=threshold)
        memory_usages.append(optimized_df.memory_usage(deep=True).sum() / 1024**2)
    return thresholds, memory_usages

def plot_memory_usage(thresholds, memory_usages):
    """
    Plots memory usage versus thresholds.
    """
    plt.figure(figsize=(10, 6))
    plt.plot(thresholds, memory_usages, marker='o', linestyle='-')
    plt.title("Memory Usage vs. Threshold")
    plt.xlabel("Threshold")
    plt.ylabel("Memory Usage (MB)")
    plt.grid(True)
    plt.show()

def analyze_sequence_data(df_sequences):
    """
    Analyzes RNA sequence data.
    """
    # Basic information
    print(f"Total sequences: {len(df_sequences)}")
    print(f"Available columns: {df_sequences.columns.tolist()}")
    
    # Sequence analysis
    if 'sequence' in df_sequences.columns:
        # Distribution of sequence lengths
        seq_lengths = df_sequences['sequence'].apply(len)
        print(f"\nSequence length statistics:")
        print(f"Minimum: {seq_lengths.min()}")
        print(f"Maximum: {seq_lengths.max()}")
        print(f"Average: {seq_lengths.mean():.2f}")
        
        # Nucleotide count
        nucleotides = ['A', 'C', 'G', 'U']
        nucleotide_counts = {n: df_sequences['sequence'].str.count(n).sum() for n in nucleotides}
        total_nucleotides = sum(nucleotide_counts.values())
        
        print("\nNucleotide distribution:")
        for n, count in nucleotide_counts.items():
            print(f"{n}: {count} ({count/total_nucleotides*100:.2f}%)")
    
    return df_sequences

def analyze_label_data(df_labels):
    """
    Analyzes 3D coordinate data (labels).
    """
    print(f"Total entries in labels: {len(df_labels)}")
    print(f"Available columns: {df_labels.columns.tolist()}")
    
    # Analysis of 3D coordinates if available
    coord_columns = [col for col in df_labels.columns if col.startswith(('x_', 'y_', 'z_'))]
    if coord_columns:
        print(f"\nCoordinate columns found: {len(coord_columns)}")
        
        # Basic statistics of coordinates
        for i in range(1, 6):  # For the 5 possible structures
            x_col = f'x_{i}'
            y_col = f'y_{i}'
            z_col = f'z_{i}'
            
            if x_col in df_labels.columns and y_col in df_labels.columns and z_col in df_labels.columns:
                print(f"\nStatistics for structure {i}:")
                print(f"X - Mean: {df_labels[x_col].mean():.2f}, Std: {df_labels[x_col].std():.2f}")
                print(f"Y - Mean: {df_labels[y_col].mean():.2f}, Std: {df_labels[y_col].std():.2f}")
                print(f"Z - Mean: {df_labels[z_col].mean():.2f}, Std: {df_labels[z_col].std():.2f}")
    
    return df_labels

def create_submission_template(test_df, sample_submission_df):
    """
    Creates a submission template based on test data.
    """
    # Check if sample_submission.csv is available
    if sample_submission_df is None:
        print("Sample submission file not found. Creating a new template.")
        
        # Create a new DataFrame for submission
        submission_df = pd.DataFrame()
        
        # Example code to fill the template (adjust as needed)
        ids = []
        resnames = []
        resids = []
        
        for _, row in test_df.iterrows():
            sequence = row['sequence']
            target_id = row['target_id']
            
            for i, nucleotide in enumerate(sequence, 1):
                ids.append(f"{target_id}_{i}")
                resnames.append(nucleotide)
                resids.append(i)
        
        submission_df['ID'] = ids
        submission_df['resname'] = resnames
        submission_df['resid'] = resids
        
        # Add coordinate columns (5 structures)
        for i in range(1, 6):
            submission_df[f'x_{i}'] = 0.0
            submission_df[f'y_{i}'] = 0.0
            submission_df[f'z_{i}'] = 0.0
    else:
        submission_df = sample_submission_df.copy()
        print("Submission template created based on the provided example.")
    
    return submission_df

def main():
    start_time = time.time()
    
    # Load main data
    print("Loading main data...")
    main_data = load_main_data()
    
    # Check which files were loaded
    print("\nLoaded files:")
    for file_name, df in main_data.items():
        print(f"- {file_name}: {df.shape if df is not None else 'Not found'}")
    
    # Analyze training sequence data
    if "train_sequences.csv" in main_data:
        print("\n===== Training Sequences Analysis =====")
        analyze_sequence_data(main_data["train_sequences.csv"])
    
    # Analyze training label data
    if "train_labels.csv" in main_data:
        print("\n===== Training Labels Analysis =====")
        analyze_label_data(main_data["train_labels.csv"])
    
    # Check for duplicates in training data
    if "train_sequences.csv" in main_data:
        print("\nChecking for duplicates in training sequences...")
        check_duplicates(main_data["train_sequences.csv"])
    
    # Create submission template
    if "test_sequences.csv" in main_data:
        print("\nCreating submission template...")
        submission_template = create_submission_template(
            main_data["test_sequences.csv"],
            main_data.get("sample_submission.csv")
        )
        print(f"Submission template shape: {submission_template.shape}")
        print(f"First rows of the submission template:")
        print(submission_template.head())
    
    # Calculate execution time
    end_time = time.time()
    print(f"\nRuntime: {end_time - start_time:.2f} seconds")
    
    return main_data

if __name__ == '__main__':
    main_data = main()
